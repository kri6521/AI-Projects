{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"rkuo2000/uecfood256\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTrcuIwazgGs",
        "outputId": "46790648-b8fa-42c7-859c-7c2f88ce168e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/rkuo2000/uecfood256?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.94G/3.94G [00:46<00:00, 91.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/rkuo2000/uecfood256/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dataset_path = \"/content/uecfood256\"\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(dataset_path):\n",
        "    print(\"‚úÖ Directory exists:\", dataset_path)\n",
        "    print(\"üìÇ Files:\", os.listdir(dataset_path))\n",
        "else:\n",
        "    print(\"‚ùå Directory not found!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTd1n88N1n9D",
        "outputId": "08ad8f5e-b1e7-41e4-8d82-09272dcced47"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Directory exists: /content/uecfood256\n",
            "üìÇ Files: ['UECFOOD256', '1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dataset_path = \"/content/uecfood256/UECFOOD256\"\n",
        "print(\"üìÇ Files in UECFOOD256:\", os.listdir(dataset_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfGCVz1R2lcD",
        "outputId": "61585575-e689-4563-d6b4-a9e4f5c39344"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Files in UECFOOD256: ['62', '248', '218', '255', '14', '225', '148', '183', '44', '74', '231', '81', '115', '171', '247', '17', '213', '138', '169', '142', '88', '27', '112', '66', '233', '56', '11', '84', '25', '217', '132', '140', '178', '214', '194', '220', '207', '216', '72', '40', '99', '134', '116', '246', '154', '73', '252', '245', '172', '121', '201', '187', '23', '98', '2', '166', '170', '147', '90', '230', '143', '52', '181', '137', '202', '5', '3', '128', '110', '253', '58', '163', '222', '168', '111', '7', '173', '156', '224', '6', '145', '242', '68', '113', '48', '199', '186', '109', '30', '175', '223', '118', '26', '176', '122', '254', '31', '209', '76', '24', '10', '125', '50', '241', '19', '182', '4', 'README.txt', '193', '102', '155', '235', '54', '184', '37', '249', '139', '9', '135', '195', '65', '18', '117', '119', '200', '1', '97', '188', '36', '211', '41', '146', '108', '161', '185', '151', '159', '20', '114', '234', '150', '55', '86', '256', '251', '15', '244', '167', '69', '104', '232', '196', '226', '165', '157', '243', '71', '126', '123', '160', '229', '94', '129', '190', '197', '120', '162', '237', '57', '85', '149', '205', '83', '153', '70', '238', '212', '77', '179', '33', '78', '67', '46', '227', '105', '240', '177', '43', '228', '100', '93', '39', '127', '131', '124', '63', '198', '133', '16', '189', '38', '204', '219', '49', '136', '22', '59', '141', '75', '82', '152', '107', '206', '203', '192', '174', '91', '180', '239', '51', '87', '215', '221', '101', '250', '42', '34', '32', '96', '60', '210', '208', '106', '45', '35', '29', '89', '164', '191', '8', '53', '79', '13', '61', '130', '28', '103', '21', 'category.txt', '236', '95', '12', '144', '92', '64', '47', '158', '80']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define paths\n",
        "data_dir = \"/content/uecfood256/UECFOOD256\"\n",
        "train_dir = \"food_dataset/train\"\n",
        "test_dir = \"food_dataset/test\"\n",
        "\n",
        "# Create train/test folders\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Get all food categories\n",
        "food_classes = os.listdir(data_dir)\n",
        "\n",
        "# Split and move images\n",
        "for food in food_classes:\n",
        "    # Check if it's a directory before proceeding\n",
        "    if os.path.isdir(os.path.join(data_dir, food)):\n",
        "        # Updated path to read images from the correct directory\n",
        "        images = os.listdir(os.path.join(data_dir, food))\n",
        "\n",
        "        # Check if the directory contains any images\n",
        "        if len(images) > 0: # This check makes sure we have images to split\n",
        "            train_images, test_images = train_test_split(images, test_size=0.2, random_state=42)\n",
        "\n",
        "            os.makedirs(os.path.join(train_dir, food), exist_ok=True)\n",
        "            os.makedirs(os.path.join(test_dir, food), exist_ok=True)\n",
        "\n",
        "            for img in train_images:\n",
        "                shutil.move(os.path.join(data_dir, food, img), os.path.join(train_dir, food, img))\n",
        "\n",
        "            for img in test_images:\n",
        "                shutil.move(os.path.join(data_dir, food, img), os.path.join(test_dir, food, img))\n",
        "        else:\n",
        "            print(f\"Skipping directory '{food}' as it contains no images.\")\n",
        "\n",
        "print(\"‚úÖ Dataset is ready with train/test split!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bILyWYpOzhUU",
        "outputId": "711b3b42-d29c-4b7c-fc57-304895c59c04"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping directory '62' as it contains no images.\n",
            "Skipping directory '248' as it contains no images.\n",
            "Skipping directory '218' as it contains no images.\n",
            "Skipping directory '255' as it contains no images.\n",
            "Skipping directory '14' as it contains no images.\n",
            "Skipping directory '225' as it contains no images.\n",
            "Skipping directory '148' as it contains no images.\n",
            "Skipping directory '183' as it contains no images.\n",
            "Skipping directory '44' as it contains no images.\n",
            "Skipping directory '74' as it contains no images.\n",
            "Skipping directory '231' as it contains no images.\n",
            "Skipping directory '81' as it contains no images.\n",
            "Skipping directory '115' as it contains no images.\n",
            "Skipping directory '171' as it contains no images.\n",
            "Skipping directory '247' as it contains no images.\n",
            "Skipping directory '17' as it contains no images.\n",
            "Skipping directory '213' as it contains no images.\n",
            "Skipping directory '138' as it contains no images.\n",
            "Skipping directory '169' as it contains no images.\n",
            "Skipping directory '142' as it contains no images.\n",
            "Skipping directory '88' as it contains no images.\n",
            "Skipping directory '27' as it contains no images.\n",
            "Skipping directory '112' as it contains no images.\n",
            "Skipping directory '66' as it contains no images.\n",
            "Skipping directory '233' as it contains no images.\n",
            "Skipping directory '56' as it contains no images.\n",
            "Skipping directory '11' as it contains no images.\n",
            "Skipping directory '84' as it contains no images.\n",
            "Skipping directory '25' as it contains no images.\n",
            "Skipping directory '217' as it contains no images.\n",
            "Skipping directory '132' as it contains no images.\n",
            "Skipping directory '140' as it contains no images.\n",
            "Skipping directory '178' as it contains no images.\n",
            "Skipping directory '214' as it contains no images.\n",
            "Skipping directory '194' as it contains no images.\n",
            "Skipping directory '220' as it contains no images.\n",
            "Skipping directory '207' as it contains no images.\n",
            "Skipping directory '216' as it contains no images.\n",
            "Skipping directory '72' as it contains no images.\n",
            "Skipping directory '40' as it contains no images.\n",
            "Skipping directory '99' as it contains no images.\n",
            "Skipping directory '134' as it contains no images.\n",
            "Skipping directory '116' as it contains no images.\n",
            "Skipping directory '246' as it contains no images.\n",
            "Skipping directory '154' as it contains no images.\n",
            "Skipping directory '73' as it contains no images.\n",
            "Skipping directory '252' as it contains no images.\n",
            "Skipping directory '245' as it contains no images.\n",
            "Skipping directory '172' as it contains no images.\n",
            "Skipping directory '121' as it contains no images.\n",
            "Skipping directory '201' as it contains no images.\n",
            "Skipping directory '187' as it contains no images.\n",
            "Skipping directory '23' as it contains no images.\n",
            "Skipping directory '98' as it contains no images.\n",
            "Skipping directory '2' as it contains no images.\n",
            "Skipping directory '166' as it contains no images.\n",
            "Skipping directory '170' as it contains no images.\n",
            "Skipping directory '147' as it contains no images.\n",
            "Skipping directory '90' as it contains no images.\n",
            "Skipping directory '230' as it contains no images.\n",
            "Skipping directory '143' as it contains no images.\n",
            "Skipping directory '52' as it contains no images.\n",
            "Skipping directory '181' as it contains no images.\n",
            "Skipping directory '137' as it contains no images.\n",
            "Skipping directory '202' as it contains no images.\n",
            "Skipping directory '5' as it contains no images.\n",
            "Skipping directory '3' as it contains no images.\n",
            "Skipping directory '128' as it contains no images.\n",
            "Skipping directory '110' as it contains no images.\n",
            "Skipping directory '253' as it contains no images.\n",
            "Skipping directory '58' as it contains no images.\n",
            "Skipping directory '163' as it contains no images.\n",
            "Skipping directory '222' as it contains no images.\n",
            "Skipping directory '168' as it contains no images.\n",
            "Skipping directory '111' as it contains no images.\n",
            "Skipping directory '7' as it contains no images.\n",
            "Skipping directory '173' as it contains no images.\n",
            "Skipping directory '156' as it contains no images.\n",
            "Skipping directory '224' as it contains no images.\n",
            "Skipping directory '6' as it contains no images.\n",
            "Skipping directory '145' as it contains no images.\n",
            "Skipping directory '242' as it contains no images.\n",
            "Skipping directory '68' as it contains no images.\n",
            "Skipping directory '113' as it contains no images.\n",
            "Skipping directory '48' as it contains no images.\n",
            "Skipping directory '199' as it contains no images.\n",
            "Skipping directory '186' as it contains no images.\n",
            "Skipping directory '109' as it contains no images.\n",
            "Skipping directory '30' as it contains no images.\n",
            "Skipping directory '175' as it contains no images.\n",
            "Skipping directory '223' as it contains no images.\n",
            "Skipping directory '118' as it contains no images.\n",
            "Skipping directory '26' as it contains no images.\n",
            "Skipping directory '176' as it contains no images.\n",
            "Skipping directory '122' as it contains no images.\n",
            "Skipping directory '254' as it contains no images.\n",
            "Skipping directory '31' as it contains no images.\n",
            "Skipping directory '209' as it contains no images.\n",
            "Skipping directory '76' as it contains no images.\n",
            "Skipping directory '24' as it contains no images.\n",
            "Skipping directory '10' as it contains no images.\n",
            "Skipping directory '125' as it contains no images.\n",
            "Skipping directory '50' as it contains no images.\n",
            "Skipping directory '241' as it contains no images.\n",
            "Skipping directory '19' as it contains no images.\n",
            "Skipping directory '182' as it contains no images.\n",
            "Skipping directory '4' as it contains no images.\n",
            "‚úÖ Dataset is ready with train/test split!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "train_dataset = ImageFolder(root=train_dir, transform=transform)\n",
        "test_dataset = ImageFolder(root=test_dir, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(train_dataset)} training images and {len(test_dataset)} test images.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWpT7jKG0ZE2",
        "outputId": "7d0c13cf-4b82-4a86-9653-942e704dc6bf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 25011 training images and 6384 test images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load ResNet-50\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, len(train_dataset.classes))  # Modify last layer\n",
        "model.to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train model\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Save trained model\n",
        "torch.save(model.state_dict(), \"food_model.pth\")\n",
        "print(\"‚úÖ Model training completed and saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adYGwke40aee",
        "outputId": "8fc598c4-05ee-40d5-a6e9-b2310fa7d4c1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 136MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 5.3183\n",
            "Epoch 2, Loss: 4.7579\n",
            "Epoch 3, Loss: 4.2387\n",
            "Epoch 4, Loss: 3.7707\n",
            "Epoch 5, Loss: 3.3377\n",
            "‚úÖ Model training completed and saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB8qH208B8ZJ",
        "outputId": "3ae880eb-ee0b-4e68-a633-0864504c10bc"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', '10', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '11', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '12', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '13', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '14', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '15', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '16', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '17', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '18', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '19', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '2', '20', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '21', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '22', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '23', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '24', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '25', '250', '251', '252', '253', '254', '255', '256', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '8', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '9', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read category mapping file (UECFOOD256/category.txt)\n",
        "def load_class_labels(file_path):\n",
        "    class_mapping = {}\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) >= 2:\n",
        "                class_id = parts[0]  # Numeric ID as string\n",
        "                class_name = \" \".join(parts[1:])  # Join the rest as food name\n",
        "                class_mapping[class_id] = class_name\n",
        "    return class_mapping\n",
        "\n",
        "# Load the class names from the dataset\n",
        "category_file = \"/content/uecfood256/UECFOOD256/category.txt\"  # Adjust path as needed\n",
        "class_labels = load_class_labels(category_file)\n",
        "print(class_labels)  # Check the mapping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-gteaWpJLjE",
        "outputId": "84bd6247-3b99-471e-84ca-2093879c56aa"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'name', '1': 'rice', '2': 'eels on rice', '3': 'pilaf', '4': \"chicken-'n'-egg on rice\", '5': 'pork cutlet on rice', '6': 'beef curry', '7': 'sushi', '8': 'chicken rice', '9': 'fried rice', '10': 'tempura bowl', '11': 'bibimbap', '12': 'toast', '13': 'croissant', '14': 'roll bread', '15': 'raisin bread', '16': 'chip butty', '17': 'hamburger', '18': 'pizza', '19': 'sandwiches', '20': 'udon noodle', '21': 'tempura udon', '22': 'soba noodle', '23': 'ramen noodle', '24': 'beef noodle', '25': 'tensin noodle', '26': 'fried noodle', '27': 'spaghetti', '28': 'Japanese-style pancake', '29': 'takoyaki', '30': 'gratin', '31': 'sauteed vegetables', '32': 'croquette', '33': 'grilled eggplant', '34': 'sauteed spinach', '35': 'vegetable tempura', '36': 'miso soup', '37': 'potage', '38': 'sausage', '39': 'oden', '40': 'omelet', '41': 'ganmodoki', '42': 'jiaozi', '43': 'stew', '44': 'teriyaki grilled fish', '45': 'fried fish', '46': 'grilled salmon', '47': 'salmon meuniere', '48': 'sashimi', '49': 'grilled pacific saury', '50': 'sukiyaki', '51': 'sweet and sour pork', '52': 'lightly roasted fish', '53': 'steamed egg hotchpotch', '54': 'tempura', '55': 'fried chicken', '56': 'sirloin cutlet', '57': 'nanbanzuke', '58': 'boiled fish', '59': 'seasoned beef with potatoes', '60': 'hambarg steak', '61': 'steak', '62': 'dried fish', '63': 'ginger pork saute', '64': 'spicy chili-flavored tofu', '65': 'yakitori', '66': 'cabbage roll', '67': 'omelet', '68': 'egg sunny-side up', '69': 'natto', '70': 'cold tofu', '71': 'egg roll', '72': 'chilled noodle', '73': 'stir-fried beef and peppers', '74': 'simmered pork', '75': 'boiled chicken and vegetables', '76': 'sashimi bowl', '77': 'sushi bowl', '78': 'fish-shaped pancake with bean jam', '79': 'shrimp with chill source', '80': 'roast chicken', '81': 'steamed meat dumpling', '82': 'omelet with fried rice', '83': 'cutlet curry', '84': 'spaghetti meat sauce', '85': 'fried shrimp', '86': 'potato salad', '87': 'green salad', '88': 'macaroni salad', '89': 'Japanese tofu and vegetable chowder', '90': 'pork miso soup', '91': 'chinese soup', '92': 'beef bowl', '93': 'kinpira-style sauteed burdock', '94': 'rice ball', '95': 'pizza toast', '96': 'dipping noodles', '97': 'hot dog', '98': 'french fries', '99': 'mixed rice', '100': 'goya chanpuru', '101': 'green curry', '102': 'okinawa soba', '103': 'mango pudding', '104': 'almond jelly', '105': 'jjigae', '106': 'dak galbi', '107': 'dry curry', '108': 'kamameshi', '109': 'rice vermicelli', '110': 'paella', '111': 'tanmen', '112': 'kushikatu', '113': 'yellow curry', '114': 'pancake', '115': 'champon', '116': 'crape', '117': 'tiramisu', '118': 'waffle', '119': 'rare cheese cake', '120': 'shortcake', '121': 'chop suey', '122': 'twice cooked pork', '123': 'mushroom risotto', '124': 'samul', '125': 'zoni', '126': 'french toast', '127': 'fine white noodles', '128': 'minestrone', '129': 'pot au feu', '130': 'chicken nugget', '131': 'namero', '132': 'french bread', '133': 'rice gruel', '134': 'broiled eel bowl', '135': 'clear soup', '136': 'yudofu', '137': 'mozuku', '138': 'inarizushi', '139': 'pork loin cutlet', '140': 'pork fillet cutlet', '141': 'chicken cutlet', '142': 'ham cutlet', '143': 'minced meat cutlet', '144': 'thinly sliced raw horsemeat', '145': 'bagel', '146': 'scone', '147': 'tortilla', '148': 'tacos', '149': 'nachos', '150': 'meat loaf', '151': 'scrambled egg', '152': 'rice gratin', '153': 'lasagna', '154': 'Caesar salad', '155': 'oatmeal', '156': 'fried pork dumplings served in soup', '157': 'oshiruko', '158': 'muffin', '159': 'popcorn', '160': 'cream puff', '161': 'doughnut', '162': 'apple pie', '163': 'parfait', '164': 'fried pork in scoop', '165': 'lamb kebabs', '166': 'dish consisting of stir-fried potato, eggplant and green pepper', '167': 'roast duck', '168': 'hot pot', '169': 'pork belly', '170': 'xiao long bao', '171': 'moon cake', '172': 'custard tart', '173': 'beef noodle soup', '174': 'pork cutlet', '175': 'minced pork rice', '176': 'fish ball soup', '177': 'oyster omelette', '178': 'glutinous oil rice', '179': 'trunip pudding', '180': 'stinky tofu', '181': 'lemon fig jelly', '182': 'khao soi', '183': 'Sour prawn soup', '184': 'Thai papaya salad', '185': 'boned, sliced Hainan-style chicken with marinated rice', '186': 'hot and sour, fish and vegetable ragout', '187': 'stir-fried mixed vegetables', '188': 'beef in oyster sauce', '189': 'pork satay', '190': 'spicy chicken salad', '191': 'noodles with fish curry', '192': 'Pork Sticky Noodles', '193': 'Pork with lemon', '194': 'stewed pork leg', '195': 'charcoal-boiled pork neck', '196': 'fried mussel pancakes', '197': 'Deep Fried Chicken Wing', '198': 'Barbecued red pork in sauce with rice', '199': 'Rice with roast duck', '200': 'Rice crispy pork', '201': 'Wonton soup', '202': 'Chicken Rice Curry With Coconut', '203': 'Crispy Noodles', '204': 'Egg Noodle In Chicken Yellow Curry', '205': 'coconut milk soup', '206': 'pho', '207': 'Hue beef rice vermicelli soup', '208': 'Vermicelli noodles with snails', '209': 'Fried spring rolls', '210': 'Steamed rice roll', '211': 'Shrimp patties', '212': 'ball shaped bun with pork', '213': 'Coconut milk-flavored crepes with shrimp and beef', '214': 'Small steamed savory rice pancake', '215': 'Glutinous Rice Balls', '216': 'loco moco', '217': 'haupia', '218': 'malasada', '219': 'laulau', '220': 'spam musubi', '221': 'oxtail soup', '222': 'adobo', '223': 'lumpia', '224': 'brownie', '225': 'churro', '226': 'jambalaya', '227': 'nasi goreng', '228': 'ayam goreng', '229': 'ayam bakar', '230': 'bubur ayam', '231': 'gulai', '232': 'laksa', '233': 'mie ayam', '234': 'mie goreng', '235': 'nasi campur', '236': 'nasi padang', '237': 'nasi uduk', '238': 'babi guling', '239': 'kaya toast', '240': 'bak kut teh', '241': 'curry puff', '242': 'chow mein', '243': 'zha jiang mian', '244': 'kung pao chicken', '245': 'crullers', '246': 'eggplant with garlic sauce', '247': 'three cup chicken', '248': 'bean curd family style', '249': 'salt & pepper fried shrimp with shell', '250': 'baked salmon', '251': 'braised pork meat ball with napa cabbage', '252': 'winter melon soup', '253': 'steamed spareribs', '254': 'chinese pumpkin pie', '255': 'eight treasure rice', '256': 'hot & sour soup'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Example: Manually assigned calorie information (You can expand this)\n",
        "calorie_info = {\n",
        "    \"rice\": 200,\n",
        "    \"miso soup\": 80,\n",
        "    \"grilled fish\": 250,\n",
        "    \"rolled egg\": 150,\n",
        "    \"natto\": 100,\n",
        "    \"eggplant miso soup\": 90,\n",
        "    \"boiled spinach\": 50,\n",
        "    \"tempura\": 350,\n",
        "    \"croquette\": 400,\n",
        "    \"yakitori\": 250,\n",
        "    \"sushi\": 200,\n",
        "    \"ramen\": 450,\n",
        "    \"burger\": 500,\n",
        "    \"pizza\": 285,\n",
        "    \"pancakes\": 350,\n",
        "    \"french fries\": 365,\n",
        "    \"fried rice\": 350,\n",
        "    \"donuts\": 450,\n",
        "    \"hot dog\": 290,\n",
        "    \"steak\": 679,\n",
        "    \"salad\": 150,\n",
        "    \"tacos\": 250,\n",
        "    \"spaghetti bolognese\": 380,\n",
        "    \"chicken wings\": 450,\n",
        "    \"gyoza\": 300,\n",
        "    \"curry rice\": 400,\n",
        "    \"tonkatsu\": 500,\n",
        "    \"udon\": 350,\n",
        "    \"soba\": 320,\n",
        "    \"stewed pork leg\": 600,\n",
        "    \"grilled chicken\": 300,\n",
        "    \"fish and chips\": 700,\n",
        "    \"sandwich\": 350,\n",
        "    \"fried chicken\": 500,\n",
        "    \"dim sum\": 280,\n",
        "    \"cheesecake\": 450,\n",
        "    \"pudding\": 220,\n",
        "    \"yakisoba\": 400,\n",
        "    \"omelette rice\": 450,\n",
        "    \"grilled eel\": 450,\n",
        "    \"clam chowder\": 250,\n",
        "    \"chicken curry\": 550,\n",
        "    \"hamburger steak\": 500,\n",
        "    \"bibimbap\": 600,\n",
        "    \"sashimi\": 150,\n",
        "    \"beef bowl\": 700,\n",
        "    \"chow mein\": 500,\n",
        "    \"hot pot\": 450,\n",
        "    \"malasada\": 350,\n",
        "    \"spring rolls\": 200,\n",
        "    \"kimchi fried rice\": 400,\n",
        "    \"pasta carbonara\": 600,\n",
        "    \"lasagna\": 700,\n",
        "    \"bruschetta\": 170,\n",
        "    \"tiramisu\": 450,\n",
        "    \"shawarma\": 600,\n",
        "    \"falafel\": 400,\n",
        "    \"kebab\": 550,\n",
        "    \"hummus\": 250\n",
        "}\n",
        "\n",
        "# Function to predict food from an image\n",
        "def predict_food(image_path, model, class_labels):\n",
        "    model.eval()\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Load and preprocess image\n",
        "    image = Image.open(image_path)\n",
        "    image = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Run model inference\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        predicted_class_id = torch.argmax(output, 1).item()\n",
        "\n",
        "    # Convert class ID to actual food name\n",
        "    predicted_class_id_str = str(predicted_class_id + 1)  # Ensure it's a string & adjust index\n",
        "    predicted_food = class_labels.get(predicted_class_id_str, \"Unknown\")\n",
        "\n",
        "    # Get calorie estimate\n",
        "    estimated_calories = calorie_info.get(predicted_food.lower(), \"Unknown\")\n",
        "\n",
        "    return predicted_food, estimated_calories\n",
        "\n",
        "#Test the function with an image\n",
        "image_path = \"/content/uecfood256/1/UECFOOD256/10/10899.jpg\"\n",
        "predicted_food, estimated_calories = predict_food(image_path, model, class_labels)\n",
        "\n",
        "#Display results\n",
        "print(f\"üçΩÔ∏è Predicted Food: {predicted_food}\")\n",
        "print(f\"üî• Estimated Calories: {estimated_calories} kcal\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wng-ijUn0o1N",
        "outputId": "7084566c-6dfb-47c6-f2d6-1a8c517c7706"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üçΩÔ∏è Predicted Food: malasada\n",
            "üî• Estimated Calories: 350 kcal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sISrWgEaGi-w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}